{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hoXQcab_5om"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "!pip install shap\n",
        "import shap\n",
        "from itertools import combinations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate_and_plot_roc(df, target_column, feature_sets, parameter_grid, n_splits=7):\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    metrics_list = []\n",
        "\n",
        "    for feature_columns in feature_sets:\n",
        "\n",
        "        metrics = {\n",
        "            'Feature Set': [', '.join(feature_columns)],\n",
        "            'AUC': [],\n",
        "            'Accuracy': [],\n",
        "            'Precision': [],\n",
        "            'Recall': [],\n",
        "            'F1 Score': [],\n",
        "            'Specificity': [],\n",
        "            'NPV': []\n",
        "        }\n",
        "\n",
        "\n",
        "        X = df[feature_columns]\n",
        "        y = df[target_column]\n",
        "\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        auc_scores = []\n",
        "        accuracy_scores = []\n",
        "        precision_scores = []\n",
        "        recall_scores = []\n",
        "        f1_scores = []\n",
        "        specificity_scores = []\n",
        "        npv_scores = []\n",
        "        tprs = []\n",
        "        mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "\n",
        "        for train_index, test_index in kf.split(X_scaled):\n",
        "            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "\n",
        "            grid_search = GridSearchCV(MLPClassifier(max_iter=2000), param_grid=parameter_grid, cv=7, scoring='roc_auc')\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_mlp_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "            y_pred_prob = best_mlp_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "            tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
        "            tprs[-1][0] = 0.0\n",
        "\n",
        "\n",
        "            roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "            auc_scores.append(roc_auc)\n",
        "\n",
        "\n",
        "            y_pred = best_mlp_model.predict(X_test)\n",
        "            accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "            precision_scores.append(precision_score(y_test, y_pred))\n",
        "            recall_scores.append(recall_score(y_test, y_pred))\n",
        "            f1_scores.append(f1_score(y_test, y_pred))\n",
        "\n",
        "            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "            specificity = tn / (tn + fp)\n",
        "            npv = tn / (tn + fn)\n",
        "            specificity_scores.append(specificity)\n",
        "            npv_scores.append(npv)\n",
        "\n",
        
        "        mean_tpr = np.mean(tprs, axis=0)\n",
        "        mean_auc = np.mean(auc_scores)\n",
        "\n",
        "\n",
        "        plt.plot(mean_fpr, mean_tpr, label=f'{\", \".join(feature_columns)} (AUC = {mean_auc:.2f})')\n",
        "\n",
        
        "        metrics['AUC'].append(mean_auc)\n",
        "        metrics['Accuracy'].append(np.mean(accuracy_scores))\n",
        "        metrics['Precision'].append(np.mean(precision_scores))\n",
        "        metrics['Recall'].append(np.mean(recall_scores))\n",
        "        metrics['F1 Score'].append(np.mean(f1_scores))\n",
        "        metrics['Specificity'].append(np.mean(specificity_scores))\n",
        "        metrics['NPV'].append(np.mean(npv_scores))\n",
        "\n",
        "        metrics_list.append(metrics)\n",
        "\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random Guessing')\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        
        "    evaluation_metrics = pd.concat([pd.DataFrame(m) for m in metrics_list], ignore_index=True)\n",
        "\n",
        
        "    evaluation_metrics.to_excel('evaluation_metrics.xlsx', index=False)\n",
        "\n",
        "    return evaluation_metrics\n"
      ],
      "metadata": {
        "id": "0GNwidc9p1IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        
        "feature_sets = [\n",
        "    ['p-tau217', 'NfL', 'Aβ42/Aβ40 ratio', 'GFAP', 'Sex','Age','Education'],\n",
        "    ['p-tau217', 'NfL', 'Aβ42/Aβ40 ratio', 'GFAP'],\n",
        "    ['NfL'],\n",
        "    ['p-tau217'],\n",
        "    ['Aβ42/Aβ40 ratio'],\n",
        "    ['GFAP'],\n",
        "\n",
        "]\n",
        "\n",
        "parameter_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': [0.0001, 0.001],\n",
        "    'learning_rate': ['constant']\n",
        "}\n",
        "\n",
        "train_evaluate_and_plot_roc(df_cn_vs, 'diagnosis', feature_sets, parameter_grid, n_splits=7)\n",
        "\n",
        
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pHDwYzryEnBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "X = df_cn_vs[feature_sets[0]]\n",
        "y = df_cn_vs['Diagnosis']\n",
        "\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "kf = KFold(n_splits=7, shuffle=True, random_state=42)\n",
        "for train_index, test_index in kf.split(X_scaled):\n",
        "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "\n",
        "    grid_search = GridSearchCV(MLPClassifier(max_iter=2000), param_grid=parameter_grid, cv=7, scoring='roc_auc')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    best_mlp_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "    explainer = shap.KernelExplainer(best_mlp_model.predict_proba, X_train)\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "\n",
        "    shap.summary_plot(shap_values[1], X_test, feature_names=feature_sets[0])\n",
        "\n",
        "\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tHRISFb3qA9W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
